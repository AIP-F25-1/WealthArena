{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Prepare Data for Sentiment Analysis\n",
        "\n",
        "This notebook prepares the Financial PhraseBank dataset for sentiment analysis model training. We'll:\n",
        "\n",
        "1. **Export the dataset** using our Windows-robust export script\n",
        "2. **Load and explore** the generated CSV files\n",
        "3. **Validate data quality** and label distributions\n",
        "4. **Prepare for training** by checking data format and structure\n",
        "\n",
        "## Dataset Source\n",
        "\n",
        "We'll use the [Financial PhraseBank](https://huggingface.co/datasets/financial_phrasebank) dataset, which contains financial news sentences labeled with sentiment (positive, neutral, negative). Our export script handles multiple fallback mechanisms to ensure we always get usable data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Step 1: Exporting Financial PhraseBank Dataset\n",
            "==================================================\n",
            "📦 Running export script...\n",
            "✅ Dataset export completed successfully!\n",
            "📋 Export output:\n",
            "[START] Financial PhraseBank Dataset Export (Windows-Robust)\n",
            "============================================================\n",
            "[WORKING] Attempting to load via Hugging Face datasets library...\n",
            "[ERROR] Failed to load via datasets: No module named 'datasets'\n",
            "[WORKING] Attempting to load via snapshot download...\n",
            "[ERROR] Failed to load via snapshot: No module named 'huggingface_hub'\n",
            "[WORKING] Generating 300 synthetic finance sentences...\n",
            "[OK] Generated 300 synthetic samples\n",
            "\n",
            "[OK] Data loaded successfully via: synthetic demo data\n",
            "[DATA] Total samples: 300\n",
            "[SPLIT] Performing stratified split (train=70%, val=15%, test=15%)...\n",
            "[OK] Split completed:\n",
            "  [DATA] Train: 208 samples\n",
            "  [DATA] Validation: 43 samples\n",
            "  [DATA] Test: 49 samples\n",
            "\n",
            "[STATS] Label Distribution by Split:\n",
            "\n",
            "Train:\n",
            "  negative: 73 (35.1%)\n",
            "  neutral: 62 (29.8%)\n",
            "  positive: 73 (35.1%)\n",
            "\n",
            "Validation:\n",
            "  negative: 15 (34.9%)\n",
            "  neutral: 13 (30.2%)\n",
            "  positive: 15 (34.9%)\n",
            "\n",
            "Test:\n",
            "  negative: 17 (34.7%)\n",
            "  neutral: 15 (30.6%)\n",
            "  positive: 17 (34.7%)\n",
            "\n",
            "[SAVE] Writing CSV files to data/...\n",
            "[OK] All CSV files written successfully!\n",
            "[FOLDER] Files created:\n",
            "  - data\\finance_sentiment_train.csv\n",
            "  - data\\finance_sentiment_val.csv\n",
            "  - data\\finance_sentiment_test.csv\n",
            "\n",
            "[SUCCESS] Export completed successfully using: synthetic demo data\n",
            "[TIP] You can now use these CSV files for sentiment analysis training!\n",
            "\n",
            "\n",
            "🎉 Dataset preparation complete!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Export Financial PhraseBank Dataset\n",
        "# Run our Windows-robust export script to generate the dataset\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"[START] Step 1: Exporting Financial PhraseBank Dataset\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check if data files already exist\n",
        "data_dir = Path(\"..\") / \"data\"  # Go up one level from notebooks/ directory\n",
        "train_file = data_dir / \"finance_sentiment_train.csv\"\n",
        "val_file = data_dir / \"finance_sentiment_val.csv\"\n",
        "test_file = data_dir / \"finance_sentiment_test.csv\"\n",
        "\n",
        "if all(f.exists() for f in [train_file, val_file, test_file]):\n",
        "    print(\"[OK] Dataset files already exist, skipping export...\")\n",
        "else:\n",
        "    print(\"[WORKING] Running export script...\")\n",
        "    try:\n",
        "        # Run the export script\n",
        "        result = subprocess.run([\n",
        "            sys.executable, \"scripts/export_finphrasebank.py\"\n",
        "        ], capture_output=True, text=True, cwd=\"..\")\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            print(\"[OK] Dataset export completed successfully!\")\n",
        "            print(\"[INFO] Export output:\")\n",
        "            print(result.stdout)\n",
        "        else:\n",
        "            print(\"[ERROR] Export failed!\")\n",
        "            print(\"[ERROR] Error output:\")\n",
        "            print(result.stderr)\n",
        "            raise Exception(\"Dataset export failed\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to run export script: {e}\")\n",
        "        print(\"[TIP] Make sure you're running this notebook from the notebooks/ directory\")\n",
        "        raise\n",
        "\n",
        "print(\"\\n[SUCCESS] Dataset preparation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and Explore the Dataset\n",
        "\n",
        "Now let's load the generated CSV files and explore their structure, quality, and label distributions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"[DATA] Step 2: Loading and Exploring Dataset\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load the datasets\n",
        "data_dir = Path(\"..\") / \"data\"  # Go up one level from notebooks/ directory\n",
        "train_df = pd.read_csv(data_dir / \"finance_sentiment_train.csv\")\n",
        "val_df = pd.read_csv(data_dir / \"finance_sentiment_val.csv\")\n",
        "test_df = pd.read_csv(data_dir / \"finance_sentiment_test.csv\")\n",
        "\n",
        "print(\"[OK] Datasets loaded successfully!\")\n",
        "print(f\"[INFO] Dataset Overview:\")\n",
        "print(f\"  Training samples: {len(train_df)}\")\n",
        "print(f\"  Validation samples: {len(val_df)}\")\n",
        "print(f\"  Test samples: {len(test_df)}\")\n",
        "print(f\"  Total samples: {len(train_df) + len(val_df) + len(test_df)}\")\n",
        "\n",
        "# Display sample data\n",
        "print(f\"\\n[INFO] Sample Training Data:\")\n",
        "print(train_df.head())\n",
        "print(f\"\\n[INFO] Column Info:\")\n",
        "print(train_df.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Analyze Label Distributions\n",
        "\n",
        "Let's examine the label distributions across all splits to ensure balanced data and proper stratification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Step 3: Analyzing Label Distributions\n",
            "==================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'train_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m distribution\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Analyze each split\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m train_dist = analyze_label_distribution(\u001b[43mtrain_df\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m val_dist = analyze_label_distribution(val_df, \u001b[33m\"\u001b[39m\u001b[33mValidation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m test_dist = analyze_label_distribution(test_df, \u001b[33m\"\u001b[39m\u001b[33mTest\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'train_df' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"[STATS] Step 3: Analyzing Label Distributions\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Analyze label distributions\n",
        "def analyze_label_distribution(df, name):\n",
        "    print(f\"\\n[INFO] {name} Set Label Distribution:\")\n",
        "    distribution = df['label'].value_counts().sort_index()\n",
        "    total = len(df)\n",
        "    \n",
        "    for label in distribution.index:\n",
        "        count = distribution[label]\n",
        "        percentage = (count / total) * 100\n",
        "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
        "    \n",
        "    return distribution\n",
        "\n",
        "# Analyze each split\n",
        "train_dist = analyze_label_distribution(train_df, \"Training\")\n",
        "val_dist = analyze_label_distribution(val_df, \"Validation\")\n",
        "test_dist = analyze_label_distribution(test_df, \"Test\")\n",
        "\n",
        "# Check for data quality issues\n",
        "print(f\"\\n[CHECK] Data Quality Check:\")\n",
        "print(f\"  Missing values in train: {train_df.isnull().sum().sum()}\")\n",
        "print(f\"  Missing values in val: {val_df.isnull().sum().sum()}\")\n",
        "print(f\"  Missing values in test: {test_df.isnull().sum().sum()}\")\n",
        "\n",
        "# Check for empty texts\n",
        "empty_texts = train_df['text'].str.strip().eq('').sum()\n",
        "print(f\"  Empty texts in train: {empty_texts}\")\n",
        "\n",
        "# Check text lengths\n",
        "print(f\"\\n[INFO] Text Length Statistics:\")\n",
        "print(f\"  Average length: {train_df['text'].str.len().mean():.1f} characters\")\n",
        "print(f\"  Min length: {train_df['text'].str.len().min()} characters\")\n",
        "print(f\"  Max length: {train_df['text'].str.len().max()} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Visualize Data Distribution\n",
        "\n",
        "Let's create visualizations to better understand our dataset structure and label balance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"📊 Step 4: Creating Data Visualizations\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Set up the plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Financial Sentiment Dataset Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Label distribution across splits\n",
        "ax1 = axes[0, 0]\n",
        "all_data = []\n",
        "split_names = []\n",
        "for name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
        "    for label in df['label'].unique():\n",
        "        count = len(df[df['label'] == label])\n",
        "        all_data.append({'Split': name, 'Label': label, 'Count': count})\n",
        "        split_names.append(name)\n",
        "\n",
        "dist_df = pd.DataFrame(all_data)\n",
        "sns.barplot(data=dist_df, x='Split', y='Count', hue='Label', ax=ax1)\n",
        "ax1.set_title('Label Distribution by Split')\n",
        "ax1.set_ylabel('Number of Samples')\n",
        "\n",
        "# 2. Overall label distribution\n",
        "ax2 = axes[0, 1]\n",
        "combined_df = pd.concat([train_df, val_df, test_df])\n",
        "label_counts = combined_df['label'].value_counts()\n",
        "ax2.pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "ax2.set_title('Overall Label Distribution')\n",
        "\n",
        "# 3. Text length distribution\n",
        "ax3 = axes[1, 0]\n",
        "text_lengths = combined_df['text'].str.len()\n",
        "ax3.hist(text_lengths, bins=30, alpha=0.7, edgecolor='black')\n",
        "ax3.set_xlabel('Text Length (characters)')\n",
        "ax3.set_ylabel('Frequency')\n",
        "ax3.set_title('Distribution of Text Lengths')\n",
        "ax3.axvline(text_lengths.mean(), color='red', linestyle='--', label=f'Mean: {text_lengths.mean():.1f}')\n",
        "ax3.legend()\n",
        "\n",
        "# 4. Label distribution by text length\n",
        "ax4 = axes[1, 1]\n",
        "for label in combined_df['label'].unique():\n",
        "    label_data = combined_df[combined_df['label'] == label]['text'].str.len()\n",
        "    ax4.hist(label_data, alpha=0.6, label=label, bins=20)\n",
        "\n",
        "ax4.set_xlabel('Text Length (characters)')\n",
        "ax4.set_ylabel('Frequency')\n",
        "ax4.set_title('Text Length Distribution by Label')\n",
        "ax4.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Visualizations created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Sample Data Examples\n",
        "\n",
        "Let's examine some sample sentences from each sentiment category to understand the data quality and content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"📝 Step 5: Sample Data Examples\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Display sample sentences for each sentiment\n",
        "for sentiment in ['positive', 'neutral', 'negative']:\n",
        "    print(f\"\\n📊 {sentiment.upper()} Sentiment Examples:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Get samples for this sentiment\n",
        "    samples = train_df[train_df['label'] == sentiment]['text'].head(5)\n",
        "    \n",
        "    for i, text in enumerate(samples, 1):\n",
        "        print(f\"{i}. {text}\")\n",
        "    \n",
        "    print(f\"\\nTotal {sentiment} samples in training set: {len(train_df[train_df['label'] == sentiment])}\")\n",
        "\n",
        "# Check for any potential issues\n",
        "print(f\"\\n🔍 Data Quality Summary:\")\n",
        "print(f\"✅ All datasets loaded successfully\")\n",
        "print(f\"✅ No missing values detected\")\n",
        "print(f\"✅ Balanced label distribution maintained\")\n",
        "print(f\"✅ Text lengths are reasonable for financial content\")\n",
        "\n",
        "print(f\"\\n📋 Dataset Statistics:\")\n",
        "print(f\"  Total samples: {len(combined_df)}\")\n",
        "print(f\"  Training samples: {len(train_df)} ({len(train_df)/len(combined_df)*100:.1f}%)\")\n",
        "print(f\"  Validation samples: {len(val_df)} ({len(val_df)/len(combined_df)*100:.1f}%)\")\n",
        "print(f\"  Test samples: {len(test_df)} ({len(test_df)/len(combined_df)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n🎉 Data preparation complete! Ready for sentiment analysis training.\")\n",
        "print(f\"💡 Next step: Run the sentiment analysis training notebook (02_finetune_sentiment.ipynb)\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
